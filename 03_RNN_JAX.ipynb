{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fc428f-0b6d-47dc-a8f3-648645ad49f8",
   "metadata": {},
   "source": [
    "# [Train RNN - equinox](https://docs.kidger.site/equinox/examples/train_rnn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "067bdfb4-c000-45e5-8139-39baa7b87c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from typing import Sequence, Optional\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.scipy as jsp\n",
    "from jaxtyping import Array, PRNGKeyArray\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "\n",
    "from tools._dataset.datasets import make_2dspiral_dataset\n",
    "from tools._dataset.dataloader import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "21adde2d-8ca1-4f79-aaca-edebeb3750ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_floating_dtype():\n",
    "    if jax.config.jax_enable_x64:  # pyright: ignore\n",
    "        return jnp.float64\n",
    "    else:\n",
    "        return jnp.float32\n",
    "\n",
    "class RNNCell(eqx.Module, strict=True):\n",
    "    \"\"\"A single step of a Recurrent Neural Network (RNN).\n",
    "\n",
    "    !!! example\n",
    "\n",
    "        This is often used by wrapping it into a `jax.lax.scan`. For example:\n",
    "\n",
    "        ```python\n",
    "        class Model(Module):\n",
    "            cell: RNNCell\n",
    "\n",
    "            def __init__(self, **kwargs):\n",
    "                self.cell = RNNCell(**kwargs)\n",
    "\n",
    "            def __call__(self, xs):\n",
    "                scan_fn = lambda state, input: (self.cell(input, state), None)\n",
    "                init_state = jnp.zeros(self.cell.hidden_size)\n",
    "                final_state, _ = jax.lax.scan(scan_fn, init_state, xs)\n",
    "                return final_state\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    weight_ih: Array\n",
    "    weight_hh: Array\n",
    "    bias: Optional[Array]\n",
    "    input_size: int = eqx.field(static=True)\n",
    "    hidden_size: int = eqx.field(static=True)\n",
    "    use_bias: bool = eqx.field(static=True)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        use_bias: bool = True,\n",
    "        dtype=None,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        \"\"\"**Arguments:**\n",
    "\n",
    "        - `input_size`: The dimensionality of the input vector at each time step.\n",
    "        - `hidden_size`: The dimensionality of the hidden state passed along between\n",
    "            time steps.\n",
    "        - `use_bias`: Whether to add on a bias after each update.\n",
    "        - `dtype`: The dtype to use for all weights and biases in this GRU cell.\n",
    "            Defaults to either `jax.numpy.float32` or `jax.numpy.float64` depending on\n",
    "            whether JAX is in 64-bit mode.\n",
    "        - `key`: A `jax.random.PRNGKey` used to provide randomness for parameter\n",
    "            initialisation. (Keyword only argument.)\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        dtype = default_floating_dtype() if dtype is None else dtype\n",
    "        ihkey, hhkey, bkey = jr.split(key, 3)\n",
    "        lim = math.sqrt(1 / hidden_size)\n",
    "\n",
    "        self.weight_ih = jr.uniform(\n",
    "            ihkey, (hidden_size, input_size), minval=-lim, maxval=lim, dtype=dtype\n",
    "        )\n",
    "        self.weight_hh = jr.uniform(\n",
    "            hhkey, (hidden_size, hidden_size), minval=-lim, maxval=lim, dtype=dtype\n",
    "        )\n",
    "        if use_bias:\n",
    "            self.bias = jr.uniform(\n",
    "                bkey, (hidden_size,), minval=-lim, maxval=lim, dtype=dtype\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    @jax.jit\n",
    "    def __call__(\n",
    "        self, input: Array, hidden: Array, *, key: Optional[PRNGKeyArray] = None\n",
    "    ):\n",
    "        \"\"\"**Arguments:**\n",
    "\n",
    "        - `input`: The input, which should be a JAX array of shape `(input_size,)`.\n",
    "        - `hidden`: The hidden state, which should be a JAX array of shape\n",
    "            `(hidden_size,)`.\n",
    "        - `key`: Ignored; provided for compatibility with the rest of the Equinox API.\n",
    "            (Keyword only argument.)\n",
    "\n",
    "        **Returns:**\n",
    "\n",
    "        The updated hidden state, which is a JAX array of shape `(hidden_size,)`.\n",
    "        \"\"\"\n",
    "        if self.use_bias:\n",
    "            bias = self.bias\n",
    "        else:\n",
    "            bias = 0\n",
    "        h_i = self.weight_ih @ input\n",
    "        h_h = self.weight_hh @ hidden\n",
    "        new = jnn.tanh(h_i + h_h + bias)\n",
    "        print(new.shape)\n",
    "        return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d0b1bfb4-3303-4328-8cc4-298993feb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(eqx.Module, strict=True):\n",
    "    \"\"\"A single step of a Gated Recurrent Unit (GRU).\n",
    "\n",
    "    !!! example\n",
    "\n",
    "        This is often used by wrapping it into a `jax.lax.scan`. For example:\n",
    "\n",
    "        ```python\n",
    "        class Model(Module):\n",
    "            cell: GRUCell\n",
    "\n",
    "            def __init__(self, **kwargs):\n",
    "                self.cell = GRUCell(**kwargs)\n",
    "\n",
    "            def __call__(self, xs):\n",
    "                scan_fn = lambda state, input: (self.cell(input, state), None)\n",
    "                init_state = jnp.zeros(self.cell.hidden_size)\n",
    "                final_state, _ = jax.lax.scan(scan_fn, init_state, xs)\n",
    "                return final_state\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    weight_ih: Array\n",
    "    weight_hh: Array\n",
    "    bias: Optional[Array]\n",
    "    bias_n: Optional[Array]\n",
    "    input_size: int = eqx.field(static=True)\n",
    "    hidden_size: int = eqx.field(static=True)\n",
    "    use_bias: bool = eqx.field(static=True)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        use_bias: bool = True,\n",
    "        dtype=None,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        \"\"\"**Arguments:**\n",
    "\n",
    "        - `input_size`: The dimensionality of the input vector at each time step.\n",
    "        - `hidden_size`: The dimensionality of the hidden state passed along between\n",
    "            time steps.\n",
    "        - `use_bias`: Whether to add on a bias after each update.\n",
    "        - `dtype`: The dtype to use for all weights and biases in this GRU cell.\n",
    "            Defaults to either `jax.numpy.float32` or `jax.numpy.float64` depending on\n",
    "            whether JAX is in 64-bit mode.\n",
    "        - `key`: A `jax.random.PRNGKey` used to provide randomness for parameter\n",
    "            initialisation. (Keyword only argument.)\n",
    "        \"\"\"\n",
    "        dtype = default_floating_dtype() if dtype is None else dtype\n",
    "        ihkey, hhkey, bkey, bkey2 = jr.split(key, 4)\n",
    "        lim = math.sqrt(1 / hidden_size)\n",
    "\n",
    "        self.weight_ih = jr.uniform(\n",
    "            ihkey, (3 * hidden_size, input_size), minval=-lim, maxval=lim, dtype=dtype\n",
    "        )\n",
    "        self.weight_hh = jr.uniform(\n",
    "            hhkey, (3 * hidden_size, hidden_size), minval=-lim, maxval=lim, dtype=dtype\n",
    "        )\n",
    "        if use_bias:\n",
    "            self.bias = jr.uniform(\n",
    "                bkey, (3 * hidden_size,), minval=-lim, maxval=lim, dtype=dtype\n",
    "            )\n",
    "            self.bias_n = jr.uniform(\n",
    "                bkey2, (hidden_size,), minval=-lim, maxval=lim, dtype=dtype\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "            self.bias_n = None\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    @jax.named_scope(\"eqx.nn.GRUCell\")\n",
    "    def __call__(\n",
    "        self, input: Array, hidden: Array, *, key: Optional[PRNGKeyArray] = None\n",
    "    ):\n",
    "        \"\"\"**Arguments:**\n",
    "\n",
    "        - `input`: The input, which should be a JAX array of shape `(input_size,)`.\n",
    "        - `hidden`: The hidden state, which should be a JAX array of shape\n",
    "            `(hidden_size,)`.\n",
    "        - `key`: Ignored; provided for compatibility with the rest of the Equinox API.\n",
    "            (Keyword only argument.)\n",
    "\n",
    "        **Returns:**\n",
    "\n",
    "        The updated hidden state, which is a JAX array of shape `(hidden_size,)`.\n",
    "        \"\"\"\n",
    "        if self.use_bias:\n",
    "            bias = self.bias\n",
    "            bias_n = self.bias_n\n",
    "        else:\n",
    "            bias = 0\n",
    "            bias_n = 0\n",
    "        igates = jnp.split(self.weight_ih @ input + bias, 3) # 単純に効率化のために一括で行列積を取って後からsplitしている.\n",
    "        hgates = jnp.split(self.weight_hh @ hidden, 3)\n",
    "        reset = jnn.sigmoid(igates[0] + hgates[0])\n",
    "        inp = jnn.sigmoid(igates[1] + hgates[1])\n",
    "        new = jnn.tanh(igates[2] + reset * (hgates[2] + bias_n))\n",
    "        new = new + inp * (hidden - new)\n",
    "        print(new.shape)\n",
    "        return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0442ab92-d013-4eac-b090-27935ebf0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(eqx.Module):\n",
    "    cell: eqx.Module # eqx.nn.GRUCell --> eqx.Module\n",
    "    dropout: eqx.Module\n",
    "    \n",
    "    def __init__(self, in_size: int, out_size: int, dropout: float = 0.0, *, key: PRNGKeyArray):\n",
    "        ckey, dkey = jr.split(key, 2)\n",
    "        self.cell = GRUCell(in_size, out_size, key=ckey) # eqx.nn.GRUCell --> RNNCell\n",
    "        self.dropout = eqx.nn.Dropout(dropout)\n",
    "        \n",
    "    def __call__(self, hidden: Array, input: Array, key: Optional[jax.random.PRNGKey] = None):\n",
    "        def _f(carry, xs):\n",
    "            carry = self.cell(xs, carry)\n",
    "            return carry, carry\n",
    "        _, outputs = lax.scan(_f, hidden, input)\n",
    "        outputs = self.dropout(outputs, key=key)\n",
    "        return outputs[-1], outputs\n",
    "\n",
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    initial: eqx.nn.MLP\n",
    "    dropout: eqx.Module\n",
    "    rnns: Sequence[eqx.Module]\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size: int, out_size: int, hidden_size: int, width_size: int, depth: int, dropout: float = 0.0, *, key: PRNGKeyArray):\n",
    "        ikey, gkey, lkey = jr.split(key, 3)\n",
    "        self.hidden_size = in_size * hidden_size\n",
    "        self.initial = eqx.nn.MLP(in_size, self.hidden_size, width_size, depth, key=ikey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout)\n",
    "        \n",
    "        self.rnns = []\n",
    "        for _ in jnp.arange(depth):\n",
    "            gkey, _gkey = jr.split(gkey, 2)\n",
    "            self.rnns.append(RNNLayer(self.hidden_size, self.hidden_size, key=_gkey))\n",
    "        \n",
    "        self.linear = eqx.nn.Linear(self.hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, input: Array, key: Optional[jax.random.PRNGKey] = None):\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "        \n",
    "        outputs = jax.vmap(self.initial)(input) # 系列長(時間)次元に vmap\n",
    "        outputs = self.dropout(outputs, key=key)\n",
    "\n",
    "        for _rnn in self.rnns:\n",
    "            hidden, outputs = _rnn(hidden, outputs)\n",
    "            \n",
    "        hidden = self.linear(hidden)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        probs = jax.nn.sigmoid(hidden + self.bias)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448bb22e-1155-4e3c-afb4-53cf5ae62abf",
   "metadata": {},
   "source": [
    "# Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eddfcadc-c861-4cf7-abcb-960aa35d6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4096,\n",
    "    length=100,\n",
    "    out_size=1,\n",
    "    add_noise=False,\n",
    "    batch_size=32,\n",
    "    lr=3e-3,\n",
    "    steps=200,\n",
    "    hidden_size=8,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "):\n",
    "    key = jr.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key, train_model_key, test_model_key = jr.split(key, 6)\n",
    "    \n",
    "    ts, ys, _, labels, in_size = make_2dspiral_dataset(\n",
    "        dataset_size, length, add_noise, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = RNN(in_size, out_size, hidden_size, width_size, depth, dropout=0.2, key=model_key)\n",
    "    #jax.debug.print(\"model: {}\", model)\n",
    "    # Training loop like normal.\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def loss(model: eqx.Module, x: Array, y: Array, key: PRNGKeyArray):\n",
    "        batched_keys = jax.random.split(key, num=x.shape[0])\n",
    "        y = lax.expand_dims(y, dimensions=[1])\n",
    "        pred = jax.vmap(model)(x, batched_keys) # バッチ次元にvmap\n",
    "        # Binary cross-entropy\n",
    "        #jax.debug.print(\"pred: {}, y: {}\", pred.shape, y.shape)\n",
    "        #jax.debug.print(\"pred: {}, y: {}\", pred, y)\n",
    "        bxe = y * jnp.log(pred) + (1 - y) * jnp.log(1 - pred)\n",
    "        bxe = -jnp.mean(bxe)\n",
    "        acc = jnp.mean((pred > 0.5) == (y == 1))\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, key):\n",
    "        key, new_key = jax.random.split(key)\n",
    "        x, y  = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, x, y, key)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state, new_key\n",
    "\n",
    "    model = eqx.nn.inference_mode(model)\n",
    "    optim = optax.adam(lr)\n",
    "    #opt_state = optim.init(model)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array)) # こっちにすると必要な勾配計算が途切れて学習されない.\n",
    "    \n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ys, labels), batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        bxe, acc, model, opt_state, train_model_key = make_step(model, data_i, opt_state, key=train_model_key)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe.item()}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        \n",
    "    model = eqx.nn.inference_mode(model, value=False)\n",
    "    ts, ys, _, labels, _ = make_2dspiral_dataset(dataset_size, length, add_noise, key=test_data_key)\n",
    "    bxe, acc = loss(model, ys, labels, key=test_model_key)\n",
    "    print(f\"Test loss: {bxe}, Test Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0660bae2-2661-4ec2-b62d-e6c0e8874842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n",
      "Step: 0, Loss: 0.7072348594665527, Accuracy: 0.46875, Computation time: 0.3663449287414551\n",
      "Step: 1, Loss: 0.75129234790802, Accuracy: 0.375, Computation time: 0.004785776138305664\n",
      "Step: 2, Loss: 0.6932094097137451, Accuracy: 0.4375, Computation time: 0.004559040069580078\n",
      "Step: 3, Loss: 0.6912630796432495, Accuracy: 0.53125, Computation time: 0.0046541690826416016\n",
      "Step: 4, Loss: 0.6814514398574829, Accuracy: 0.59375, Computation time: 0.0046939849853515625\n",
      "Step: 5, Loss: 0.6781452894210815, Accuracy: 0.59375, Computation time: 0.004791975021362305\n",
      "Step: 6, Loss: 0.7136348485946655, Accuracy: 0.46875, Computation time: 0.004964113235473633\n",
      "Step: 7, Loss: 0.7135650515556335, Accuracy: 0.46875, Computation time: 0.004450798034667969\n",
      "Step: 8, Loss: 0.6365460157394409, Accuracy: 0.75, Computation time: 0.004570960998535156\n",
      "Step: 9, Loss: 0.6766389608383179, Accuracy: 0.59375, Computation time: 0.00452876091003418\n",
      "Step: 10, Loss: 0.6384806632995605, Accuracy: 0.71875, Computation time: 0.004611968994140625\n",
      "Step: 11, Loss: 0.718858003616333, Accuracy: 0.46875, Computation time: 0.004514932632446289\n",
      "Step: 12, Loss: 0.6979929208755493, Accuracy: 0.53125, Computation time: 0.004518747329711914\n",
      "Step: 13, Loss: 0.6867947578430176, Accuracy: 0.5625, Computation time: 0.004866838455200195\n",
      "Step: 14, Loss: 0.7089793682098389, Accuracy: 0.5, Computation time: 0.00452113151550293\n",
      "Step: 15, Loss: 0.6862562894821167, Accuracy: 0.5625, Computation time: 0.004560947418212891\n",
      "Step: 16, Loss: 0.7260187864303589, Accuracy: 0.4375, Computation time: 0.004488945007324219\n",
      "Step: 17, Loss: 0.7482503056526184, Accuracy: 0.34375, Computation time: 0.004687070846557617\n",
      "Step: 18, Loss: 0.7073643207550049, Accuracy: 0.46875, Computation time: 0.004712104797363281\n",
      "Step: 19, Loss: 0.7028353214263916, Accuracy: 0.46875, Computation time: 0.004415988922119141\n",
      "Step: 20, Loss: 0.6951042413711548, Accuracy: 0.5, Computation time: 0.0046100616455078125\n",
      "Step: 21, Loss: 0.6937363147735596, Accuracy: 0.5, Computation time: 0.0046231746673583984\n",
      "Step: 22, Loss: 0.6925381422042847, Accuracy: 0.53125, Computation time: 0.004580974578857422\n",
      "Step: 23, Loss: 0.6924965381622314, Accuracy: 0.53125, Computation time: 0.0045130252838134766\n",
      "Step: 24, Loss: 0.7000112533569336, Accuracy: 0.40625, Computation time: 0.004511833190917969\n",
      "Step: 25, Loss: 0.699140191078186, Accuracy: 0.4375, Computation time: 0.00452876091003418\n",
      "Step: 26, Loss: 0.6913613080978394, Accuracy: 0.53125, Computation time: 0.004431962966918945\n",
      "Step: 27, Loss: 0.6831046342849731, Accuracy: 0.625, Computation time: 0.004376888275146484\n",
      "Step: 28, Loss: 0.6878938674926758, Accuracy: 0.5625, Computation time: 0.004881143569946289\n",
      "Step: 29, Loss: 0.7028242945671082, Accuracy: 0.4375, Computation time: 0.004505157470703125\n",
      "Step: 30, Loss: 0.7035319805145264, Accuracy: 0.4375, Computation time: 0.004452228546142578\n",
      "Step: 31, Loss: 0.6870416402816772, Accuracy: 0.5625, Computation time: 0.0044019222259521484\n",
      "Step: 32, Loss: 0.702723503112793, Accuracy: 0.4375, Computation time: 0.004363059997558594\n",
      "Step: 33, Loss: 0.6980973482131958, Accuracy: 0.46875, Computation time: 0.004606962203979492\n",
      "Step: 34, Loss: 0.6878312826156616, Accuracy: 0.5625, Computation time: 0.004477024078369141\n",
      "Step: 35, Loss: 0.6965512037277222, Accuracy: 0.46875, Computation time: 0.00458979606628418\n",
      "Step: 36, Loss: 0.6907364130020142, Accuracy: 0.53125, Computation time: 0.004426002502441406\n",
      "Step: 37, Loss: 0.6885291337966919, Accuracy: 0.5625, Computation time: 0.004718780517578125\n",
      "Step: 38, Loss: 0.6907958984375, Accuracy: 0.53125, Computation time: 0.004590272903442383\n",
      "Step: 39, Loss: 0.6840631365776062, Accuracy: 0.65625, Computation time: 0.00440216064453125\n",
      "Step: 40, Loss: 0.7069610953330994, Accuracy: 0.25, Computation time: 0.004393100738525391\n",
      "Step: 41, Loss: 0.6897876262664795, Accuracy: 0.53125, Computation time: 0.0046291351318359375\n",
      "Step: 42, Loss: 0.6894073486328125, Accuracy: 0.53125, Computation time: 0.00439000129699707\n",
      "Step: 43, Loss: 0.6886091232299805, Accuracy: 0.53125, Computation time: 0.004516124725341797\n",
      "Step: 44, Loss: 0.6862186193466187, Accuracy: 0.625, Computation time: 0.004762172698974609\n",
      "Step: 45, Loss: 0.6887253522872925, Accuracy: 0.5, Computation time: 0.004508256912231445\n",
      "Step: 46, Loss: 0.6866786479949951, Accuracy: 0.46875, Computation time: 0.00480198860168457\n",
      "Step: 47, Loss: 0.6801788806915283, Accuracy: 0.78125, Computation time: 0.004756927490234375\n",
      "Step: 48, Loss: 0.6805122494697571, Accuracy: 0.53125, Computation time: 0.004697084426879883\n",
      "Step: 49, Loss: 0.6739461421966553, Accuracy: 0.625, Computation time: 0.0046808719635009766\n",
      "Step: 50, Loss: 0.6794508695602417, Accuracy: 0.5, Computation time: 0.004616260528564453\n",
      "Step: 51, Loss: 0.6735317707061768, Accuracy: 0.53125, Computation time: 0.0045397281646728516\n",
      "Step: 52, Loss: 0.686707615852356, Accuracy: 0.375, Computation time: 0.004667997360229492\n",
      "Step: 53, Loss: 0.6771888136863708, Accuracy: 0.46875, Computation time: 0.004706144332885742\n",
      "Step: 54, Loss: 0.675438404083252, Accuracy: 0.4375, Computation time: 0.004579782485961914\n",
      "Step: 55, Loss: 0.6781076192855835, Accuracy: 0.40625, Computation time: 0.004860877990722656\n",
      "Step: 56, Loss: 0.6661849021911621, Accuracy: 0.84375, Computation time: 0.0045697689056396484\n",
      "Step: 57, Loss: 0.6670255661010742, Accuracy: 0.90625, Computation time: 0.004705905914306641\n",
      "Step: 58, Loss: 0.662155270576477, Accuracy: 0.90625, Computation time: 0.0046710968017578125\n",
      "Step: 59, Loss: 0.6588743925094604, Accuracy: 0.9375, Computation time: 0.004524707794189453\n",
      "Step: 60, Loss: 0.6443217396736145, Accuracy: 1.0, Computation time: 0.004585981369018555\n",
      "Step: 61, Loss: 0.6444849967956543, Accuracy: 0.90625, Computation time: 0.004523038864135742\n",
      "Step: 62, Loss: 0.6462266445159912, Accuracy: 0.75, Computation time: 0.004554271697998047\n",
      "Step: 63, Loss: 0.6451572775840759, Accuracy: 0.53125, Computation time: 0.004478931427001953\n",
      "Step: 64, Loss: 0.648847222328186, Accuracy: 0.46875, Computation time: 0.0046079158782958984\n",
      "Step: 65, Loss: 0.62750244140625, Accuracy: 0.59375, Computation time: 0.004365205764770508\n",
      "Step: 66, Loss: 0.6143267154693604, Accuracy: 0.625, Computation time: 0.004336118698120117\n",
      "Step: 67, Loss: 0.636094868183136, Accuracy: 0.5, Computation time: 0.004721164703369141\n",
      "Step: 68, Loss: 0.6386646032333374, Accuracy: 0.5, Computation time: 0.004498958587646484\n",
      "Step: 69, Loss: 0.6654040813446045, Accuracy: 0.34375, Computation time: 0.004539966583251953\n",
      "Step: 70, Loss: 0.6090622544288635, Accuracy: 0.59375, Computation time: 0.0045430660247802734\n",
      "Step: 71, Loss: 0.6192126274108887, Accuracy: 0.53125, Computation time: 0.0044291019439697266\n",
      "Step: 72, Loss: 0.6325212717056274, Accuracy: 0.4375, Computation time: 0.0045011043548583984\n",
      "Step: 73, Loss: 0.6000107526779175, Accuracy: 1.0, Computation time: 0.004681825637817383\n",
      "Step: 74, Loss: 0.6029943823814392, Accuracy: 1.0, Computation time: 0.004376888275146484\n",
      "Step: 75, Loss: 0.6141180992126465, Accuracy: 1.0, Computation time: 0.0046689510345458984\n",
      "Step: 76, Loss: 0.5863521099090576, Accuracy: 1.0, Computation time: 0.004478931427001953\n",
      "Step: 77, Loss: 0.5830634832382202, Accuracy: 1.0, Computation time: 0.004518985748291016\n",
      "Step: 78, Loss: 0.5774035453796387, Accuracy: 1.0, Computation time: 0.004580974578857422\n",
      "Step: 79, Loss: 0.5661906003952026, Accuracy: 1.0, Computation time: 0.004503965377807617\n",
      "Step: 80, Loss: 0.5605790615081787, Accuracy: 1.0, Computation time: 0.0044748783111572266\n",
      "Step: 81, Loss: 0.5578696727752686, Accuracy: 1.0, Computation time: 0.0047321319580078125\n",
      "Step: 82, Loss: 0.535624623298645, Accuracy: 1.0, Computation time: 0.004533052444458008\n",
      "Step: 83, Loss: 0.5282358527183533, Accuracy: 1.0, Computation time: 0.004530906677246094\n",
      "Step: 84, Loss: 0.516261875629425, Accuracy: 1.0, Computation time: 0.004987239837646484\n",
      "Step: 85, Loss: 0.5098809003829956, Accuracy: 1.0, Computation time: 0.005560874938964844\n",
      "Step: 86, Loss: 0.5278208255767822, Accuracy: 1.0, Computation time: 0.004850864410400391\n",
      "Step: 87, Loss: 0.4936012923717499, Accuracy: 1.0, Computation time: 0.00510096549987793\n",
      "Step: 88, Loss: 0.47046345472335815, Accuracy: 1.0, Computation time: 0.004878997802734375\n",
      "Step: 89, Loss: 0.44678574800491333, Accuracy: 1.0, Computation time: 0.004556894302368164\n",
      "Step: 90, Loss: 0.4496183693408966, Accuracy: 1.0, Computation time: 0.004540920257568359\n",
      "Step: 91, Loss: 0.424084335565567, Accuracy: 1.0, Computation time: 0.004523754119873047\n",
      "Step: 92, Loss: 0.40154120326042175, Accuracy: 1.0, Computation time: 0.004690885543823242\n",
      "Step: 93, Loss: 0.4034087061882019, Accuracy: 1.0, Computation time: 0.004563093185424805\n",
      "Step: 94, Loss: 0.3848094940185547, Accuracy: 1.0, Computation time: 0.004522085189819336\n",
      "Step: 95, Loss: 0.36411595344543457, Accuracy: 1.0, Computation time: 0.0045130252838134766\n",
      "Step: 96, Loss: 0.365938663482666, Accuracy: 1.0, Computation time: 0.00439000129699707\n",
      "Step: 97, Loss: 0.34566348791122437, Accuracy: 1.0, Computation time: 0.0046160221099853516\n",
      "Step: 98, Loss: 0.33277061581611633, Accuracy: 1.0, Computation time: 0.004664897918701172\n",
      "Step: 99, Loss: 0.32790809869766235, Accuracy: 1.0, Computation time: 0.004523038864135742\n",
      "Step: 100, Loss: 0.3064119517803192, Accuracy: 1.0, Computation time: 0.004683017730712891\n",
      "Step: 101, Loss: 0.2964475452899933, Accuracy: 1.0, Computation time: 0.004597902297973633\n",
      "Step: 102, Loss: 0.2740072011947632, Accuracy: 1.0, Computation time: 0.004480123519897461\n",
      "Step: 103, Loss: 0.26713889837265015, Accuracy: 1.0, Computation time: 0.0045621395111083984\n",
      "Step: 104, Loss: 0.26733309030532837, Accuracy: 1.0, Computation time: 0.004570722579956055\n",
      "Step: 105, Loss: 0.26007404923439026, Accuracy: 1.0, Computation time: 0.004525661468505859\n",
      "Step: 106, Loss: 0.2590758204460144, Accuracy: 1.0, Computation time: 0.004478931427001953\n",
      "Step: 107, Loss: 0.24822697043418884, Accuracy: 1.0, Computation time: 0.0044291019439697266\n",
      "Step: 108, Loss: 0.23048073053359985, Accuracy: 1.0, Computation time: 0.004438877105712891\n",
      "Step: 109, Loss: 0.21358871459960938, Accuracy: 1.0, Computation time: 0.004464864730834961\n",
      "Step: 110, Loss: 0.1983107626438141, Accuracy: 1.0, Computation time: 0.0046138763427734375\n",
      "Step: 111, Loss: 0.20890741050243378, Accuracy: 1.0, Computation time: 0.0047359466552734375\n",
      "Step: 112, Loss: 0.20111894607543945, Accuracy: 1.0, Computation time: 0.004487037658691406\n",
      "Step: 113, Loss: 0.18641847372055054, Accuracy: 1.0, Computation time: 0.004520893096923828\n",
      "Step: 114, Loss: 0.17463372647762299, Accuracy: 1.0, Computation time: 0.0046520233154296875\n",
      "Step: 115, Loss: 0.18131905794143677, Accuracy: 1.0, Computation time: 0.004637002944946289\n",
      "Step: 116, Loss: 0.16206613183021545, Accuracy: 1.0, Computation time: 0.004572868347167969\n",
      "Step: 117, Loss: 0.16463902592658997, Accuracy: 1.0, Computation time: 0.0045011043548583984\n",
      "Step: 118, Loss: 0.16804936528205872, Accuracy: 1.0, Computation time: 0.00468897819519043\n",
      "Step: 119, Loss: 0.1547992080450058, Accuracy: 1.0, Computation time: 0.004628181457519531\n",
      "Step: 120, Loss: 0.15444472432136536, Accuracy: 1.0, Computation time: 0.004637002944946289\n",
      "Step: 121, Loss: 0.13571050763130188, Accuracy: 1.0, Computation time: 0.0044438838958740234\n",
      "Step: 122, Loss: 0.13650692999362946, Accuracy: 1.0, Computation time: 0.004511117935180664\n",
      "Step: 123, Loss: 0.13550682365894318, Accuracy: 1.0, Computation time: 0.004326820373535156\n",
      "Step: 124, Loss: 0.13384680449962616, Accuracy: 1.0, Computation time: 0.0050051212310791016\n",
      "Step: 125, Loss: 0.12606890499591827, Accuracy: 1.0, Computation time: 0.004559993743896484\n",
      "Step: 126, Loss: 0.12084649503231049, Accuracy: 1.0, Computation time: 0.0048639774322509766\n",
      "Step: 127, Loss: 0.12421567738056183, Accuracy: 1.0, Computation time: 0.004479169845581055\n",
      "Step: 128, Loss: 0.11636024713516235, Accuracy: 1.0, Computation time: 0.004725933074951172\n",
      "Step: 129, Loss: 0.1142907440662384, Accuracy: 1.0, Computation time: 0.0045812129974365234\n",
      "Step: 130, Loss: 0.11194661259651184, Accuracy: 1.0, Computation time: 0.004549741744995117\n",
      "Step: 131, Loss: 0.10738351941108704, Accuracy: 1.0, Computation time: 0.004762887954711914\n",
      "Step: 132, Loss: 0.10604330897331238, Accuracy: 1.0, Computation time: 0.004717826843261719\n",
      "Step: 133, Loss: 0.10486175119876862, Accuracy: 1.0, Computation time: 0.004602193832397461\n",
      "Step: 134, Loss: 0.1010894924402237, Accuracy: 1.0, Computation time: 0.0047149658203125\n",
      "Step: 135, Loss: 0.10191966593265533, Accuracy: 1.0, Computation time: 0.004579067230224609\n",
      "Step: 136, Loss: 0.0975186750292778, Accuracy: 1.0, Computation time: 0.004673004150390625\n",
      "Step: 137, Loss: 0.09664216637611389, Accuracy: 1.0, Computation time: 0.004559755325317383\n",
      "Step: 138, Loss: 0.09359577298164368, Accuracy: 1.0, Computation time: 0.004590749740600586\n",
      "Step: 139, Loss: 0.09225817024707794, Accuracy: 1.0, Computation time: 0.004630088806152344\n",
      "Step: 140, Loss: 0.09075228869915009, Accuracy: 1.0, Computation time: 0.004606962203979492\n",
      "Step: 141, Loss: 0.08785229176282883, Accuracy: 1.0, Computation time: 0.004408121109008789\n",
      "Step: 142, Loss: 0.0867774561047554, Accuracy: 1.0, Computation time: 0.00471186637878418\n",
      "Step: 143, Loss: 0.08529464900493622, Accuracy: 1.0, Computation time: 0.004436969757080078\n",
      "Step: 144, Loss: 0.08337460458278656, Accuracy: 1.0, Computation time: 0.004603862762451172\n",
      "Step: 145, Loss: 0.0819861888885498, Accuracy: 1.0, Computation time: 0.004706144332885742\n",
      "Step: 146, Loss: 0.08047065138816833, Accuracy: 1.0, Computation time: 0.004464864730834961\n",
      "Step: 147, Loss: 0.07904940098524094, Accuracy: 1.0, Computation time: 0.004591941833496094\n",
      "Step: 148, Loss: 0.07760794460773468, Accuracy: 1.0, Computation time: 0.004487276077270508\n",
      "Step: 149, Loss: 0.07626780867576599, Accuracy: 1.0, Computation time: 0.004506111145019531\n",
      "Step: 150, Loss: 0.0751640796661377, Accuracy: 1.0, Computation time: 0.0047109127044677734\n",
      "Step: 151, Loss: 0.07438655197620392, Accuracy: 1.0, Computation time: 0.004372119903564453\n",
      "Step: 152, Loss: 0.07210725545883179, Accuracy: 1.0, Computation time: 0.004557132720947266\n",
      "Step: 153, Loss: 0.07122424244880676, Accuracy: 1.0, Computation time: 0.004621982574462891\n",
      "Step: 154, Loss: 0.07121015340089798, Accuracy: 1.0, Computation time: 0.004452228546142578\n",
      "Step: 155, Loss: 0.0702163577079773, Accuracy: 1.0, Computation time: 0.0047419071197509766\n",
      "Step: 156, Loss: 0.0678856372833252, Accuracy: 1.0, Computation time: 0.004733085632324219\n",
      "Step: 157, Loss: 0.06681899726390839, Accuracy: 1.0, Computation time: 0.0045070648193359375\n",
      "Step: 158, Loss: 0.06643680483102798, Accuracy: 1.0, Computation time: 0.004558086395263672\n",
      "Step: 159, Loss: 0.06610327214002609, Accuracy: 1.0, Computation time: 0.004519939422607422\n",
      "Step: 160, Loss: 0.06467250734567642, Accuracy: 1.0, Computation time: 0.0046160221099853516\n",
      "Step: 161, Loss: 0.06382417678833008, Accuracy: 1.0, Computation time: 0.004615068435668945\n",
      "Step: 162, Loss: 0.0625133216381073, Accuracy: 1.0, Computation time: 0.0048828125\n",
      "Step: 163, Loss: 0.060914769768714905, Accuracy: 1.0, Computation time: 0.004436969757080078\n",
      "Step: 164, Loss: 0.06087233126163483, Accuracy: 1.0, Computation time: 0.005547046661376953\n",
      "Step: 165, Loss: 0.0604231096804142, Accuracy: 1.0, Computation time: 0.004904031753540039\n",
      "Step: 166, Loss: 0.05958765745162964, Accuracy: 1.0, Computation time: 0.0049130916595458984\n",
      "Step: 167, Loss: 0.0589890256524086, Accuracy: 1.0, Computation time: 0.004467010498046875\n",
      "Step: 168, Loss: 0.05733390524983406, Accuracy: 1.0, Computation time: 0.004942893981933594\n",
      "Step: 169, Loss: 0.057131245732307434, Accuracy: 1.0, Computation time: 0.004520893096923828\n",
      "Step: 170, Loss: 0.05634785443544388, Accuracy: 1.0, Computation time: 0.004647970199584961\n",
      "Step: 171, Loss: 0.055622100830078125, Accuracy: 1.0, Computation time: 0.004627227783203125\n",
      "Step: 172, Loss: 0.05493825301527977, Accuracy: 1.0, Computation time: 0.004505157470703125\n",
      "Step: 173, Loss: 0.05431995540857315, Accuracy: 1.0, Computation time: 0.004795074462890625\n",
      "Step: 174, Loss: 0.0535304993391037, Accuracy: 1.0, Computation time: 0.00459599494934082\n",
      "Step: 175, Loss: 0.05282815918326378, Accuracy: 1.0, Computation time: 0.004801034927368164\n",
      "Step: 176, Loss: 0.05221834033727646, Accuracy: 1.0, Computation time: 0.004590034484863281\n",
      "Step: 177, Loss: 0.051742590963840485, Accuracy: 1.0, Computation time: 0.004673004150390625\n",
      "Step: 178, Loss: 0.051042862236499786, Accuracy: 1.0, Computation time: 0.004601955413818359\n",
      "Step: 179, Loss: 0.05060533061623573, Accuracy: 1.0, Computation time: 0.004549980163574219\n",
      "Step: 180, Loss: 0.04988502711057663, Accuracy: 1.0, Computation time: 0.004410982131958008\n",
      "Step: 181, Loss: 0.04941735416650772, Accuracy: 1.0, Computation time: 0.004837751388549805\n",
      "Step: 182, Loss: 0.048944614827632904, Accuracy: 1.0, Computation time: 0.004431009292602539\n",
      "Step: 183, Loss: 0.0482429563999176, Accuracy: 1.0, Computation time: 0.0046520233154296875\n",
      "Step: 184, Loss: 0.04771195352077484, Accuracy: 1.0, Computation time: 0.0046999454498291016\n",
      "Step: 185, Loss: 0.0471012219786644, Accuracy: 1.0, Computation time: 0.004510164260864258\n",
      "Step: 186, Loss: 0.04671236872673035, Accuracy: 1.0, Computation time: 0.004533052444458008\n",
      "Step: 187, Loss: 0.04621847718954086, Accuracy: 1.0, Computation time: 0.0046100616455078125\n",
      "Step: 188, Loss: 0.04576468467712402, Accuracy: 1.0, Computation time: 0.004656076431274414\n",
      "Step: 189, Loss: 0.04522474855184555, Accuracy: 1.0, Computation time: 0.00477290153503418\n",
      "Step: 190, Loss: 0.04488174617290497, Accuracy: 1.0, Computation time: 0.004564046859741211\n",
      "Step: 191, Loss: 0.04439812898635864, Accuracy: 1.0, Computation time: 0.004578113555908203\n",
      "Step: 192, Loss: 0.043961051851511, Accuracy: 1.0, Computation time: 0.004575014114379883\n",
      "Step: 193, Loss: 0.04341907799243927, Accuracy: 1.0, Computation time: 0.004497051239013672\n",
      "Step: 194, Loss: 0.04313097894191742, Accuracy: 1.0, Computation time: 0.004601001739501953\n",
      "Step: 195, Loss: 0.042585939168930054, Accuracy: 1.0, Computation time: 0.004572153091430664\n",
      "Step: 196, Loss: 0.04225224629044533, Accuracy: 1.0, Computation time: 0.004930019378662109\n",
      "Step: 197, Loss: 0.04169023036956787, Accuracy: 1.0, Computation time: 0.004552125930786133\n",
      "Step: 198, Loss: 0.04138752818107605, Accuracy: 1.0, Computation time: 0.004681825637817383\n",
      "Step: 199, Loss: 0.04090094938874245, Accuracy: 1.0, Computation time: 0.004626274108886719\n",
      "(24,)\n",
      "Test loss: 0.04353778064250946, Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "eqx.clear_caches()\n",
    "jax.clear_caches()\n",
    "main() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9cf1a2-1952-48f8-bf27-1636c4147fec",
   "metadata": {},
   "source": [
    "- `class RNNLayer` の `self.cell` を `RNNCell` 　にすると失敗。 `GRUCell` なら動く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea649096-09c3-4396-87cb-f80a6bf3f26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77abf62-73d3-479d-b2fb-92f9a18fad04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
