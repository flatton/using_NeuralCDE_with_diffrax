{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fc428f-0b6d-47dc-a8f3-648645ad49f8",
   "metadata": {},
   "source": [
    "# [Train RNN - equinox](https://docs.kidger.site/equinox/examples/train_rnn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "067bdfb4-c000-45e5-8139-39baa7b87c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from typing import Tuple, Sequence, Optional\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.scipy as jsp\n",
    "from jaxtyping import Array, Float, PRNGKeyArray\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "\n",
    "from tools._dataset.datasets import SpiralDataset\n",
    "from tools._dataset.dataloader import dataloader\n",
    "from tools._model.discrete_cde import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0dcb1-cf6b-4626-9994-8d4633b3a7f7",
   "metadata": {},
   "source": [
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    initial: eqx.nn.MLP\n",
    "    rnn: DiscreteCDELayer\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size: int, out_size: int, hidden_size: int, width_size: int, depth: int, *, key: PRNGKeyArray):\n",
    "        ikey, gkey, lkey = jr.split(key, 3)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.initial = eqx.nn.MLP(in_size, hidden_size, width_size, depth, key=ikey)\n",
    "        \n",
    "        self.rnn = DiscreteCDELayer(in_size, hidden_size, width_size, depth, key=gkey)\n",
    "        \n",
    "        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, xs: Array, key: Optional[jax.random.PRNGKey] = None):\n",
    "        x0 = xs[0,:]\n",
    "        y0 = self.initial(x0) # y0\n",
    "        yT, ys = self.rnn(y0, xs)\n",
    "        logits = self.linear(yT)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        probs = jax.nn.sigmoid(logits + self.bias)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448bb22e-1155-4e3c-afb4-53cf5ae62abf",
   "metadata": {},
   "source": [
    "# Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eddfcadc-c861-4cf7-abcb-960aa35d6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4096,\n",
    "    length=100,\n",
    "    out_size=1,\n",
    "    add_noise=False,\n",
    "    batch_size=32,\n",
    "    lr=3e-3,\n",
    "    steps=100,\n",
    "    hidden_size=8,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "):\n",
    "    key = jr.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jr.split(key, 4)\n",
    "    \n",
    "    dataset = SpiralDataset(dataset_size, length, add_noise, key=train_data_key)\n",
    "    ts, ys, _, labels, in_size = dataset.make_dataset()\n",
    "\n",
    "    model = RNN(in_size, out_size, hidden_size, width_size, depth, key=model_key)\n",
    "    #jax.debug.print(\"model: {}\", model)\n",
    "    # Training loop like normal.\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def loss(model: eqx.Module, x: Array, y: Array, *, key: Optional[PRNGKeyArray] = None) -> Tuple[Float, Float]:\n",
    "        #batched_keys = jax.random.split(key, num=x.shape[0])\n",
    "        preds = jax.vmap(model)(x) # dropoutを使うときは, (x, batched_keys)\n",
    "        \n",
    "        # Binary cross-entropy\n",
    "        y = lax.expand_dims(y, dimensions=[1])\n",
    "        bxe = y * jnp.log(preds) + (1 - y) * jnp.log(1 - preds)\n",
    "        bxe = -jnp.mean(bxe)\n",
    "        acc = jnp.mean((preds > 0.5) == (y == 1))\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model: eqx.Module, data: Tuple[Array, ...], opt_state: Tuple, key: Optional[PRNGKeyArray] = None) -> Tuple[Float, Float, eqx.Module, Tuple]:\n",
    "        # key, new_key = jax.random.split(key)\n",
    "        x, y  = data\n",
    "        (bxe, acc), grads = grad_loss(model, x, y) # (model, x, y, key)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state #, new_key\n",
    "\n",
    "    model = eqx.nn.inference_mode(model)\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    \n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ys, labels), batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe.item()}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        \n",
    "    model = eqx.nn.inference_mode(model, value=False)\n",
    "    dataset = SpiralDataset(dataset_size, length, add_noise, key=train_data_key)\n",
    "    ts, ys, _, labels, _ = dataset.make_dataset()\n",
    "    bxe, acc = loss(model, ys, labels)\n",
    "    print(f\"Test loss: {bxe}, Test Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0660bae2-2661-4ec2-b62d-e6c0e8874842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 2.6379456520080566, Accuracy: 0.46875, Computation time: 0.3254389762878418\n",
      "Step: 1, Loss: 0.9420516490936279, Accuracy: 0.4375, Computation time: 0.004773855209350586\n",
      "Step: 2, Loss: 0.9228380918502808, Accuracy: 0.28125, Computation time: 0.00456690788269043\n",
      "Step: 3, Loss: 0.6698753237724304, Accuracy: 0.40625, Computation time: 0.004745960235595703\n",
      "Step: 4, Loss: 0.6057009696960449, Accuracy: 0.59375, Computation time: 0.004636049270629883\n",
      "Step: 5, Loss: 0.5476295948028564, Accuracy: 0.59375, Computation time: 0.004507780075073242\n",
      "Step: 6, Loss: 0.44034063816070557, Accuracy: 0.625, Computation time: 0.0046901702880859375\n",
      "Step: 7, Loss: 0.39380085468292236, Accuracy: 1.0, Computation time: 0.00453495979309082\n",
      "Step: 8, Loss: 0.37781476974487305, Accuracy: 1.0, Computation time: 0.004513978958129883\n",
      "Step: 9, Loss: 0.3344845771789551, Accuracy: 1.0, Computation time: 0.00455927848815918\n",
      "Step: 10, Loss: 0.28384533524513245, Accuracy: 1.0, Computation time: 0.004534006118774414\n",
      "Step: 11, Loss: 0.237959086894989, Accuracy: 1.0, Computation time: 0.0046422481536865234\n",
      "Step: 12, Loss: 0.1912018358707428, Accuracy: 1.0, Computation time: 0.004704713821411133\n",
      "Step: 13, Loss: 0.14161650836467743, Accuracy: 1.0, Computation time: 0.004562854766845703\n",
      "Step: 14, Loss: 0.1025644838809967, Accuracy: 1.0, Computation time: 0.0046231746673583984\n",
      "Step: 15, Loss: 0.06730067729949951, Accuracy: 1.0, Computation time: 0.004609823226928711\n",
      "Step: 16, Loss: 0.05419376865029335, Accuracy: 1.0, Computation time: 0.004644870758056641\n",
      "Step: 17, Loss: 0.026943782344460487, Accuracy: 1.0, Computation time: 0.00450897216796875\n",
      "Step: 18, Loss: 0.028148937970399857, Accuracy: 1.0, Computation time: 0.004490852355957031\n",
      "Step: 19, Loss: 0.014963432215154171, Accuracy: 1.0, Computation time: 0.004591226577758789\n",
      "Step: 20, Loss: 0.008636828511953354, Accuracy: 1.0, Computation time: 0.0045642852783203125\n",
      "Step: 21, Loss: 0.0028776521794497967, Accuracy: 1.0, Computation time: 0.004602193832397461\n",
      "Step: 22, Loss: 0.002172361360862851, Accuracy: 1.0, Computation time: 0.0045278072357177734\n",
      "Step: 23, Loss: 0.0012020994909107685, Accuracy: 1.0, Computation time: 0.004441976547241211\n",
      "Step: 24, Loss: 0.0011072728084400296, Accuracy: 1.0, Computation time: 0.0046308040618896484\n",
      "Step: 25, Loss: 0.0008521670242771506, Accuracy: 1.0, Computation time: 0.0044498443603515625\n",
      "Step: 26, Loss: 0.002242084126919508, Accuracy: 1.0, Computation time: 0.004536151885986328\n",
      "Step: 27, Loss: 0.0005603191675618291, Accuracy: 1.0, Computation time: 0.00450897216796875\n",
      "Step: 28, Loss: 0.0003497904690448195, Accuracy: 1.0, Computation time: 0.004529237747192383\n",
      "Step: 29, Loss: 0.0003043754259124398, Accuracy: 1.0, Computation time: 0.004479885101318359\n",
      "Step: 30, Loss: 0.00019148534920532256, Accuracy: 1.0, Computation time: 0.004571199417114258\n",
      "Step: 31, Loss: 0.00012053074169671163, Accuracy: 1.0, Computation time: 0.0044972896575927734\n",
      "Step: 32, Loss: 6.309813761617988e-05, Accuracy: 1.0, Computation time: 0.004423856735229492\n",
      "Step: 33, Loss: 6.394711090251803e-05, Accuracy: 1.0, Computation time: 0.0046558380126953125\n",
      "Step: 34, Loss: 5.078367394162342e-05, Accuracy: 1.0, Computation time: 0.004549980163574219\n",
      "Step: 35, Loss: 3.695247869472951e-05, Accuracy: 1.0, Computation time: 0.004558086395263672\n",
      "Step: 36, Loss: 3.4957207390107214e-05, Accuracy: 1.0, Computation time: 0.004405975341796875\n",
      "Step: 37, Loss: 2.7102061721961945e-05, Accuracy: 1.0, Computation time: 0.004449129104614258\n",
      "Step: 38, Loss: 2.396527270320803e-05, Accuracy: 1.0, Computation time: 0.0044040679931640625\n",
      "Step: 39, Loss: 2.3045078705763444e-05, Accuracy: 1.0, Computation time: 0.004568815231323242\n",
      "Step: 40, Loss: 2.4244671294582076e-05, Accuracy: 1.0, Computation time: 0.004554033279418945\n",
      "Step: 41, Loss: 2.672200025699567e-05, Accuracy: 1.0, Computation time: 0.004694938659667969\n",
      "Step: 42, Loss: 2.305248381162528e-05, Accuracy: 1.0, Computation time: 0.00603485107421875\n",
      "Step: 43, Loss: 2.3404541934723966e-05, Accuracy: 1.0, Computation time: 0.0045430660247802734\n",
      "Step: 44, Loss: 1.869379229901824e-05, Accuracy: 1.0, Computation time: 0.004584312438964844\n",
      "Step: 45, Loss: 1.578237788635306e-05, Accuracy: 1.0, Computation time: 0.004488945007324219\n",
      "Step: 46, Loss: 1.707509727566503e-05, Accuracy: 1.0, Computation time: 0.004695892333984375\n",
      "Step: 47, Loss: 1.788538065738976e-05, Accuracy: 1.0, Computation time: 0.004844188690185547\n",
      "Step: 48, Loss: 1.6825484635774046e-05, Accuracy: 1.0, Computation time: 0.004756927490234375\n",
      "Step: 49, Loss: 1.7790356650948524e-05, Accuracy: 1.0, Computation time: 0.004765987396240234\n",
      "Step: 50, Loss: 1.5435942259500735e-05, Accuracy: 1.0, Computation time: 0.004700183868408203\n",
      "Step: 51, Loss: 1.5327870642067865e-05, Accuracy: 1.0, Computation time: 0.004602909088134766\n",
      "Step: 52, Loss: 1.5678073395974934e-05, Accuracy: 1.0, Computation time: 0.004662036895751953\n",
      "Step: 53, Loss: 1.5709736544522457e-05, Accuracy: 1.0, Computation time: 0.004477024078369141\n",
      "Step: 54, Loss: 1.2222810255479999e-05, Accuracy: 1.0, Computation time: 0.0046389102935791016\n",
      "Step: 55, Loss: 1.5236615581670776e-05, Accuracy: 1.0, Computation time: 0.004566192626953125\n",
      "Step: 56, Loss: 1.3336681149667129e-05, Accuracy: 1.0, Computation time: 0.004599094390869141\n",
      "Step: 57, Loss: 1.53521013999125e-05, Accuracy: 1.0, Computation time: 0.004571199417114258\n",
      "Step: 58, Loss: 1.6626170690869913e-05, Accuracy: 1.0, Computation time: 0.004652738571166992\n",
      "Step: 59, Loss: 1.4988874681876041e-05, Accuracy: 1.0, Computation time: 0.004593849182128906\n",
      "Step: 60, Loss: 1.516583051852649e-05, Accuracy: 1.0, Computation time: 0.0048677921295166016\n",
      "Step: 61, Loss: 1.2945523849339224e-05, Accuracy: 1.0, Computation time: 0.004580974578857422\n",
      "Step: 62, Loss: 1.3834011951985303e-05, Accuracy: 1.0, Computation time: 0.004618406295776367\n",
      "Step: 63, Loss: 1.3591865354101174e-05, Accuracy: 1.0, Computation time: 0.00458216667175293\n",
      "Step: 64, Loss: 1.0039742846856825e-05, Accuracy: 1.0, Computation time: 0.004792690277099609\n",
      "Step: 65, Loss: 1.5303667169064283e-05, Accuracy: 1.0, Computation time: 0.0045909881591796875\n",
      "Step: 66, Loss: 1.333109503320884e-05, Accuracy: 1.0, Computation time: 0.00471806526184082\n",
      "Step: 67, Loss: 1.539865115773864e-05, Accuracy: 1.0, Computation time: 0.004569053649902344\n",
      "Step: 68, Loss: 1.5979818272171542e-05, Accuracy: 1.0, Computation time: 0.0046007633209228516\n",
      "Step: 69, Loss: 1.3457754903356545e-05, Accuracy: 1.0, Computation time: 0.0045812129974365234\n",
      "Step: 70, Loss: 1.2733171388390474e-05, Accuracy: 1.0, Computation time: 0.004575014114379883\n",
      "Step: 71, Loss: 1.2057025742251426e-05, Accuracy: 1.0, Computation time: 0.004523038864135742\n",
      "Step: 72, Loss: 1.3347855201573111e-05, Accuracy: 1.0, Computation time: 0.0045320987701416016\n",
      "Step: 73, Loss: 1.2744349078275263e-05, Accuracy: 1.0, Computation time: 0.004556179046630859\n",
      "Step: 74, Loss: 1.3852640222467016e-05, Accuracy: 1.0, Computation time: 0.004504203796386719\n",
      "Step: 75, Loss: 1.3156010027159937e-05, Accuracy: 1.0, Computation time: 0.00458979606628418\n",
      "Step: 76, Loss: 1.1794389138231054e-05, Accuracy: 1.0, Computation time: 0.004634857177734375\n",
      "Step: 77, Loss: 1.0559442671365105e-05, Accuracy: 1.0, Computation time: 0.004540920257568359\n",
      "Step: 78, Loss: 1.2470534784370102e-05, Accuracy: 1.0, Computation time: 0.004623889923095703\n",
      "Step: 79, Loss: 1.2604657968040556e-05, Accuracy: 1.0, Computation time: 0.004621744155883789\n",
      "Step: 80, Loss: 1.4808190826443024e-05, Accuracy: 1.0, Computation time: 0.004595041275024414\n",
      "Step: 81, Loss: 1.4294092579802964e-05, Accuracy: 1.0, Computation time: 0.00460505485534668\n",
      "Step: 82, Loss: 1.4307128367363475e-05, Accuracy: 1.0, Computation time: 0.004662036895751953\n",
      "Step: 83, Loss: 1.3431679690256715e-05, Accuracy: 1.0, Computation time: 0.004658937454223633\n",
      "Step: 84, Loss: 1.5853147488087416e-05, Accuracy: 1.0, Computation time: 0.004590272903442383\n",
      "Step: 85, Loss: 1.157644874183461e-05, Accuracy: 1.0, Computation time: 0.004667758941650391\n",
      "Step: 86, Loss: 1.5670610082452185e-05, Accuracy: 1.0, Computation time: 0.004725933074951172\n",
      "Step: 87, Loss: 1.2634465747396462e-05, Accuracy: 1.0, Computation time: 0.004683017730712891\n",
      "Step: 88, Loss: 1.36123471747851e-05, Accuracy: 1.0, Computation time: 0.0046138763427734375\n",
      "Step: 89, Loss: 1.3517361367121339e-05, Accuracy: 1.0, Computation time: 0.004722118377685547\n",
      "Step: 90, Loss: 1.2537579095805995e-05, Accuracy: 1.0, Computation time: 0.004737138748168945\n",
      "Step: 91, Loss: 1.1300773621769622e-05, Accuracy: 1.0, Computation time: 0.004707813262939453\n",
      "Step: 92, Loss: 1.5516010535066016e-05, Accuracy: 1.0, Computation time: 0.004750251770019531\n",
      "Step: 93, Loss: 1.3556463272834662e-05, Accuracy: 1.0, Computation time: 0.004670858383178711\n",
      "Step: 94, Loss: 1.1792523764597718e-05, Accuracy: 1.0, Computation time: 0.004608869552612305\n",
      "Step: 95, Loss: 1.3116887203068472e-05, Accuracy: 1.0, Computation time: 0.004637241363525391\n",
      "Step: 96, Loss: 1.238672120962292e-05, Accuracy: 1.0, Computation time: 0.004700183868408203\n",
      "Step: 97, Loss: 1.387313113809796e-05, Accuracy: 1.0, Computation time: 0.004647016525268555\n",
      "Step: 98, Loss: 1.566874561831355e-05, Accuracy: 1.0, Computation time: 0.004662990570068359\n",
      "Step: 99, Loss: 1.5355828509200364e-05, Accuracy: 1.0, Computation time: 0.004637241363525391\n",
      "Test loss: 1.31236211018404e-05, Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "eqx.clear_caches()\n",
    "jax.clear_caches()\n",
    "main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fea230-8934-49d4-9cdb-64f74d65ecbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
