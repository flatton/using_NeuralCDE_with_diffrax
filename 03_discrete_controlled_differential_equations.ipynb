{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fc428f-0b6d-47dc-a8f3-648645ad49f8",
   "metadata": {},
   "source": [
    "# [Train RNN - equinox](https://docs.kidger.site/equinox/examples/train_rnn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067bdfb4-c000-45e5-8139-39baa7b87c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from typing import Tuple, Sequence, Optional\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.scipy as jsp\n",
    "from jaxtyping import Array, Float, PRNGKeyArray\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "\n",
    "from tools._dataset.datasets import SpiralDataset\n",
    "from tools._dataset.dataloader import dataloader\n",
    "from tools._model.discrete_cde import RNN\n",
    "from tools._loss.cross_entropy import bce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0dcb1-cf6b-4626-9994-8d4633b3a7f7",
   "metadata": {},
   "source": [
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    initial: eqx.nn.MLP\n",
    "    rnn: DiscreteCDELayer\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size: int, out_size: int, hidden_size: int, width_size: int, depth: int, *, key: PRNGKeyArray):\n",
    "        ikey, gkey, lkey = jr.split(key, 3)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.initial = eqx.nn.MLP(in_size, hidden_size, width_size, depth, key=ikey)\n",
    "        \n",
    "        self.rnn = DiscreteCDELayer(in_size, hidden_size, width_size, depth, key=gkey)\n",
    "        \n",
    "        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, xs: Array, key: Optional[jax.random.PRNGKey] = None):\n",
    "        x0 = xs[0,:]\n",
    "        y0 = self.initial(x0) # y0\n",
    "        yT, ys = self.rnn(y0, xs)\n",
    "        logits = self.linear(yT)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        probs = jax.nn.sigmoid(logits + self.bias)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448bb22e-1155-4e3c-afb4-53cf5ae62abf",
   "metadata": {},
   "source": [
    "# Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddfcadc-c861-4cf7-abcb-960aa35d6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4096,\n",
    "    length=100,\n",
    "    out_size=1,\n",
    "    add_noise=False,\n",
    "    batch_size=32,\n",
    "    lr=3e-3,\n",
    "    steps=100,\n",
    "    hidden_size=8,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "):\n",
    "    key = jr.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jr.split(key, 4)\n",
    "    \n",
    "    dataset = SpiralDataset(dataset_size, length, add_noise, key=train_data_key)\n",
    "    ts, _, coeffs, labels, in_size = dataset.make_dataset()\n",
    "\n",
    "    model = RNN(in_size, out_size, hidden_size, width_size, depth, key=model_key)\n",
    "    \n",
    "    # Training loop like normal.\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(bce_loss, has_aux=True)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model: eqx.Module, data: Array, opt_state: Tuple, *args, key:PRNGKeyArray) -> Tuple[Float, Float, eqx.Module, Tuple]:\n",
    "        ts, labels, *coeffs = data\n",
    "        (bxe, acc), grads = grad_loss(model, (ts, coeffs), labels, *args, key=key)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    model = eqx.nn.inference_mode(model, value=False)\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "    loader_key, _loader_key = jr.split(loader_key, 2)\n",
    "    for step, data in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=_loader_key)\n",
    "    ):\n",
    "        loader_key, _loader_key = jr.split(loader_key, 2)\n",
    "        start = time.time()\n",
    "        bxe, acc, model, opt_state = make_step(model, data, opt_state, key=_loader_key)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "\n",
    "    model = eqx.nn.inference_mode(model)\n",
    "    test_data_key, inference_key = jr.split(test_data_key, 2)\n",
    "    dataset = SpiralDataset(dataset_size, length, add_noise, key=test_data_key)\n",
    "    ts, _, coeffs, labels, _ = dataset.make_dataset()\n",
    "    bxe, acc = bce_loss(model, (ts, coeffs), labels, key=inference_key)\n",
    "    print(f\"Test loss: {bxe}, Test Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0660bae2-2661-4ec2-b62d-e6c0e8874842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 2.630345344543457, Accuracy: 0.46875, Computation time: 0.5296788215637207\n",
      "Step: 1, Loss: 0.6006730794906616, Accuracy: 0.6875, Computation time: 0.4188117980957031\n",
      "Step: 2, Loss: 1.2619365453720093, Accuracy: 0.5, Computation time: 0.4128890037536621\n",
      "Step: 3, Loss: 0.687018632888794, Accuracy: 0.53125, Computation time: 0.0051801204681396484\n",
      "Step: 4, Loss: 0.5568324327468872, Accuracy: 0.5, Computation time: 0.005151987075805664\n",
      "Step: 5, Loss: 0.8020071387290955, Accuracy: 0.4375, Computation time: 0.004984140396118164\n",
      "Step: 6, Loss: 0.6486579179763794, Accuracy: 0.46875, Computation time: 0.005139827728271484\n",
      "Step: 7, Loss: 0.4808724522590637, Accuracy: 0.9375, Computation time: 0.005090951919555664\n",
      "Step: 8, Loss: 0.3821321725845337, Accuracy: 1.0, Computation time: 0.005079984664916992\n",
      "Step: 9, Loss: 0.4018373489379883, Accuracy: 0.875, Computation time: 0.005011081695556641\n",
      "Step: 10, Loss: 0.4957374930381775, Accuracy: 0.625, Computation time: 0.004930019378662109\n",
      "Step: 11, Loss: 0.2581866383552551, Accuracy: 1.0, Computation time: 0.005007028579711914\n",
      "Step: 12, Loss: 0.26126450300216675, Accuracy: 1.0, Computation time: 0.004996776580810547\n",
      "Step: 13, Loss: 0.1911618411540985, Accuracy: 1.0, Computation time: 0.005040884017944336\n",
      "Step: 14, Loss: 0.1566179394721985, Accuracy: 1.0, Computation time: 0.005021810531616211\n",
      "Step: 15, Loss: 0.1421298384666443, Accuracy: 1.0, Computation time: 0.005038022994995117\n",
      "Step: 16, Loss: 0.15297047793865204, Accuracy: 1.0, Computation time: 0.0050029754638671875\n",
      "Step: 17, Loss: 0.11979948729276657, Accuracy: 1.0, Computation time: 0.004961252212524414\n",
      "Step: 18, Loss: 0.06693008542060852, Accuracy: 1.0, Computation time: 0.0049381256103515625\n",
      "Step: 19, Loss: 0.04327767714858055, Accuracy: 1.0, Computation time: 0.0050089359283447266\n",
      "Step: 20, Loss: 0.03077130764722824, Accuracy: 1.0, Computation time: 0.005045890808105469\n",
      "Step: 21, Loss: 0.021871183067560196, Accuracy: 1.0, Computation time: 0.005095958709716797\n",
      "Step: 22, Loss: 0.019308507442474365, Accuracy: 1.0, Computation time: 0.0050182342529296875\n",
      "Step: 23, Loss: 0.016539352014660835, Accuracy: 1.0, Computation time: 0.005014181137084961\n",
      "Step: 24, Loss: 0.012257665395736694, Accuracy: 1.0, Computation time: 0.005063295364379883\n",
      "Step: 25, Loss: 0.008012983947992325, Accuracy: 1.0, Computation time: 0.0050449371337890625\n",
      "Step: 26, Loss: 0.009311939589679241, Accuracy: 1.0, Computation time: 0.005018949508666992\n",
      "Step: 27, Loss: 0.00591376144438982, Accuracy: 1.0, Computation time: 0.00494694709777832\n",
      "Step: 28, Loss: 0.007673901971429586, Accuracy: 1.0, Computation time: 0.004967927932739258\n",
      "Step: 29, Loss: 0.006756385788321495, Accuracy: 1.0, Computation time: 0.005072832107543945\n",
      "Step: 30, Loss: 0.006459049880504608, Accuracy: 1.0, Computation time: 0.005134105682373047\n",
      "Step: 31, Loss: 0.004545525647699833, Accuracy: 1.0, Computation time: 0.004986286163330078\n",
      "Step: 32, Loss: 0.005025043152272701, Accuracy: 1.0, Computation time: 0.005008220672607422\n",
      "Step: 33, Loss: 0.0035081212408840656, Accuracy: 1.0, Computation time: 0.004913806915283203\n",
      "Step: 34, Loss: 0.0034073013812303543, Accuracy: 1.0, Computation time: 0.004931926727294922\n",
      "Step: 35, Loss: 0.00298628443852067, Accuracy: 1.0, Computation time: 0.0058400630950927734\n",
      "Step: 36, Loss: 0.0024022869765758514, Accuracy: 1.0, Computation time: 0.005337953567504883\n",
      "Step: 37, Loss: 0.002048002788797021, Accuracy: 1.0, Computation time: 0.006285905838012695\n",
      "Step: 38, Loss: 0.0019730585627257824, Accuracy: 1.0, Computation time: 0.005579948425292969\n",
      "Step: 39, Loss: 0.0018008551560342312, Accuracy: 1.0, Computation time: 0.00520014762878418\n",
      "Step: 40, Loss: 0.0017316474113613367, Accuracy: 1.0, Computation time: 0.005093097686767578\n",
      "Step: 41, Loss: 0.0016333764651790261, Accuracy: 1.0, Computation time: 0.0049860477447509766\n",
      "Step: 42, Loss: 0.001273459754884243, Accuracy: 1.0, Computation time: 0.00499415397644043\n",
      "Step: 43, Loss: 0.0007830850081518292, Accuracy: 1.0, Computation time: 0.004952907562255859\n",
      "Step: 44, Loss: 0.0012668331619352102, Accuracy: 1.0, Computation time: 0.00506591796875\n",
      "Step: 45, Loss: 0.0008851751335896552, Accuracy: 1.0, Computation time: 0.004995822906494141\n",
      "Step: 46, Loss: 0.0010007910896092653, Accuracy: 1.0, Computation time: 0.0051310062408447266\n",
      "Step: 47, Loss: 0.0006281506503000855, Accuracy: 1.0, Computation time: 0.0050199031829833984\n",
      "Step: 48, Loss: 0.0008478140225633979, Accuracy: 1.0, Computation time: 0.0049591064453125\n",
      "Step: 49, Loss: 0.0006615373422391713, Accuracy: 1.0, Computation time: 0.0050449371337890625\n",
      "Step: 50, Loss: 0.0006525003118440509, Accuracy: 1.0, Computation time: 0.004972219467163086\n",
      "Step: 51, Loss: 0.0006868739146739244, Accuracy: 1.0, Computation time: 0.004994869232177734\n",
      "Step: 52, Loss: 0.00041139539098367095, Accuracy: 1.0, Computation time: 0.004931926727294922\n",
      "Step: 53, Loss: 0.00036883659777231514, Accuracy: 1.0, Computation time: 0.005151987075805664\n",
      "Step: 54, Loss: 0.0004692566581070423, Accuracy: 1.0, Computation time: 0.0051500797271728516\n",
      "Step: 55, Loss: 0.00035808945540338755, Accuracy: 1.0, Computation time: 0.005019187927246094\n",
      "Step: 56, Loss: 0.00030990957748144865, Accuracy: 1.0, Computation time: 0.0050220489501953125\n",
      "Step: 57, Loss: 0.0002860360546037555, Accuracy: 1.0, Computation time: 0.00513005256652832\n",
      "Step: 58, Loss: 0.0003415087121538818, Accuracy: 1.0, Computation time: 0.0050067901611328125\n",
      "Step: 59, Loss: 0.0003737025544978678, Accuracy: 1.0, Computation time: 0.004791736602783203\n",
      "Step: 60, Loss: 0.000230500940233469, Accuracy: 1.0, Computation time: 0.005051136016845703\n",
      "Step: 61, Loss: 0.0003283813130110502, Accuracy: 1.0, Computation time: 0.005043983459472656\n",
      "Step: 62, Loss: 0.0003238902136217803, Accuracy: 1.0, Computation time: 0.004936933517456055\n",
      "Step: 63, Loss: 0.00028220698004588485, Accuracy: 1.0, Computation time: 0.005056142807006836\n",
      "Step: 64, Loss: 0.00024136394495144486, Accuracy: 1.0, Computation time: 0.005038738250732422\n",
      "Step: 65, Loss: 0.0002543803711887449, Accuracy: 1.0, Computation time: 0.005108833312988281\n",
      "Step: 66, Loss: 0.00029785087099298835, Accuracy: 1.0, Computation time: 0.005036115646362305\n",
      "Step: 67, Loss: 0.00021792721236124635, Accuracy: 1.0, Computation time: 0.005070924758911133\n",
      "Step: 68, Loss: 0.00022247122251428664, Accuracy: 1.0, Computation time: 0.005085945129394531\n",
      "Step: 69, Loss: 0.0001880433119367808, Accuracy: 1.0, Computation time: 0.004900932312011719\n",
      "Step: 70, Loss: 0.0002287223469465971, Accuracy: 1.0, Computation time: 0.005146026611328125\n",
      "Step: 71, Loss: 0.00020646238408517092, Accuracy: 1.0, Computation time: 0.004921913146972656\n",
      "Step: 72, Loss: 0.0002261426270706579, Accuracy: 1.0, Computation time: 0.004786014556884766\n",
      "Step: 73, Loss: 0.00018707953859120607, Accuracy: 1.0, Computation time: 0.004986763000488281\n",
      "Step: 74, Loss: 0.00012607270036824048, Accuracy: 1.0, Computation time: 0.004802703857421875\n",
      "Step: 75, Loss: 0.00019329122733324766, Accuracy: 1.0, Computation time: 0.00479578971862793\n",
      "Step: 76, Loss: 0.00014464001287706196, Accuracy: 1.0, Computation time: 0.00486302375793457\n",
      "Step: 77, Loss: 0.00015278378850780427, Accuracy: 1.0, Computation time: 0.004887104034423828\n",
      "Step: 78, Loss: 0.0001634515356272459, Accuracy: 1.0, Computation time: 0.004834175109863281\n",
      "Step: 79, Loss: 0.00017032635514624417, Accuracy: 1.0, Computation time: 0.005020856857299805\n",
      "Step: 80, Loss: 0.00014435261255130172, Accuracy: 1.0, Computation time: 0.0049588680267333984\n",
      "Step: 81, Loss: 0.00014520660624839365, Accuracy: 1.0, Computation time: 0.005084991455078125\n",
      "Step: 82, Loss: 0.0001585094869369641, Accuracy: 1.0, Computation time: 0.004998922348022461\n",
      "Step: 83, Loss: 0.00013928326370660216, Accuracy: 1.0, Computation time: 0.0049169063568115234\n",
      "Step: 84, Loss: 0.00015526344941463321, Accuracy: 1.0, Computation time: 0.004974842071533203\n",
      "Step: 85, Loss: 0.00013000235776416957, Accuracy: 1.0, Computation time: 0.004949092864990234\n",
      "Step: 86, Loss: 0.00014584418386220932, Accuracy: 1.0, Computation time: 0.004976749420166016\n",
      "Step: 87, Loss: 0.0001540898228995502, Accuracy: 1.0, Computation time: 0.004995107650756836\n",
      "Step: 88, Loss: 0.000104713391920086, Accuracy: 1.0, Computation time: 0.004978179931640625\n",
      "Step: 89, Loss: 0.00015072591486386955, Accuracy: 1.0, Computation time: 0.005009889602661133\n",
      "Step: 90, Loss: 0.0001717062696116045, Accuracy: 1.0, Computation time: 0.0050258636474609375\n",
      "Step: 91, Loss: 9.291150490753353e-05, Accuracy: 1.0, Computation time: 0.005106210708618164\n",
      "Step: 92, Loss: 0.00012981709733139724, Accuracy: 1.0, Computation time: 0.005015850067138672\n",
      "Step: 93, Loss: 8.740601333556697e-05, Accuracy: 1.0, Computation time: 0.004929065704345703\n",
      "Step: 94, Loss: 0.00011026569700334221, Accuracy: 1.0, Computation time: 0.0049571990966796875\n",
      "Step: 95, Loss: 0.00012082602916052565, Accuracy: 1.0, Computation time: 0.005026102066040039\n",
      "Step: 96, Loss: 9.393664367962629e-05, Accuracy: 1.0, Computation time: 0.0050449371337890625\n",
      "Step: 97, Loss: 9.180689812637866e-05, Accuracy: 1.0, Computation time: 0.0050029754638671875\n",
      "Step: 98, Loss: 0.00010312094673281536, Accuracy: 1.0, Computation time: 0.00496673583984375\n",
      "Step: 99, Loss: 0.00010323018068447709, Accuracy: 1.0, Computation time: 0.004988908767700195\n",
      "Test loss: 0.00011512692435644567, Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "eqx.clear_caches()\n",
    "jax.clear_caches()\n",
    "main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fea230-8934-49d4-9cdb-64f74d65ecbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
