{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b81bf40-bd03-4910-bd8d-4768b6150d34",
   "metadata": {},
   "source": [
    "# [Train RNN - equinox](https://docs.kidger.site/equinox/examples/train_rnn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d07437-f38a-4448-91f6-05d19c5633f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomoki.fujihara/Desktop/test_diffrax/test_diffrax/.venv/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:54: UserWarning: `validate_input_format` overrides an existing Pydantic `@field_validator` decorator\n",
      "  warnings.warn(f'`{k}` overrides an existing Pydantic `{existing.decorator_info.decorator_repr}` decorator')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from typing import Sequence, Tuple, Union, Callable, Optional\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.scipy as jsp\n",
    "from jaxtyping import Array, Float, PRNGKeyArray\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "\n",
    "from tools._dataset.datasets import MNISTStrokeDataset\n",
    "from tools._dataset.dataloader import dataloader_ununiformed_sequence\n",
    "from tools._model.discrete_cde import RNN\n",
    "from tools._loss.cross_entropy import nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18237e2d-cf83-4998-a958-f6ab66e405db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d845f-2bba-4617-add9-1ce7ac97a85d",
   "metadata": {},
   "source": [
    "# Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43cb3a69-ed28-4fa0-ab69-338841a3a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=128,\n",
    "    noise_ratio=0.1,\n",
    "    input_format='point_sequence',\n",
    "    interpolation='cubic',\n",
    "    out_size=10,\n",
    "    batch_size=32,\n",
    "    lr=3e-3,\n",
    "    steps=50,\n",
    "    hidden_size=8,\n",
    "    width_size=128,\n",
    "    depth=3,\n",
    "    seed=5678,\n",
    "):\n",
    "    key = jr.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key, test_loss_key = jr.split(key, 5)\n",
    "\n",
    "    dataset = MNISTStrokeDataset(dataset_size=dataset_size, mode_train=True, input_format=input_format, noise_ratio=noise_ratio, interpolation=interpolation, key=train_data_key)\n",
    "    ts, _, coeffs, labels, in_size = dataset.make_dataset()\n",
    "    \n",
    "    model = RNN(in_size, out_size, hidden_size, width_size, depth, interpolation=interpolation, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(nll_loss, has_aux=True)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model: eqx.Module, data: Array, opt_state: Tuple, *args, key:PRNGKeyArray) -> Tuple[Float, Float, eqx.Module, Tuple]:\n",
    "        ts, *coeffs, labels = data\n",
    "        (xe, acc), grads = grad_loss(model, (ts, coeffs), labels, *args, key=key)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return xe, acc, model, opt_state\n",
    "\n",
    "    model = eqx.nn.inference_mode(model, value=False)\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "    loader_key, _loader_key = jr.split(loader_key, 2)\n",
    "    for step, data in zip(\n",
    "        range(steps), dataloader_ununiformed_sequence(((ts, *coeffs), labels), batch_size, key=_loader_key)\n",
    "    ):\n",
    "        loader_key, _loader_key = jr.split(loader_key, 2)\n",
    "        start = time.time()\n",
    "        xe, acc, model, opt_state = make_step(model, data, opt_state, out_size, key=_loader_key)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {xe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        \n",
    "    model = eqx.nn.inference_mode(model)\n",
    "    dataset = MNISTStrokeDataset(dataset_size=256, mode_train=False, input_format=input_format, noise_ratio=noise_ratio, interpolation=interpolation, key=test_data_key)\n",
    "    ts, _, coeffs, labels, _ = dataset.make_dataset()\n",
    "    list_xe = []\n",
    "    list_acc = []\n",
    "    for data in zip(ts, *coeffs, labels):\n",
    "        data = [jnp.expand_dims(x, axis=0) for x in data]\n",
    "        _ts, *_coeffs, _label = data\n",
    "        xe, acc = nll_loss(model, (_ts, _coeffs), _label, out_size, key=test_loss_key)\n",
    "        list_xe.append(lax.stop_gradient(xe))\n",
    "        list_acc.append(lax.stop_gradient(acc))\n",
    "    print(f\"Test loss: {jnp.mean(jnp.array(list_xe))}, Test Accuracy: {jnp.mean(jnp.array(list_acc))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20eda4c-fd1a-4a2e-a3f0-91c5ba98ec0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 128/128 [00:07<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 4.699886798858643, Accuracy: 0.0625, Computation time: 0.6800119876861572\n",
      "Step: 1, Loss: 11.674543380737305, Accuracy: 0.09375, Computation time: 0.7505300045013428\n",
      "Step: 2, Loss: 9.841757774353027, Accuracy: 0.03125, Computation time: 0.6397271156311035\n",
      "Step: 3, Loss: 7.363530158996582, Accuracy: 0.21875, Computation time: 0.011176824569702148\n",
      "Step: 4, Loss: 4.607048034667969, Accuracy: 0.15625, Computation time: 0.6388082504272461\n",
      "Step: 5, Loss: 7.659836292266846, Accuracy: 0.0625, Computation time: 0.653205156326294\n",
      "Step: 6, Loss: 4.6439056396484375, Accuracy: 0.03125, Computation time: 0.619055986404419\n",
      "Step: 7, Loss: 4.378640174865723, Accuracy: 0.125, Computation time: 0.7398090362548828\n",
      "Step: 8, Loss: 3.8317275047302246, Accuracy: 0.0625, Computation time: 0.011806011199951172\n",
      "Step: 9, Loss: 2.735403299331665, Accuracy: 0.15625, Computation time: 0.011660099029541016\n",
      "Step: 10, Loss: 5.218893051147461, Accuracy: 0.21875, Computation time: 0.011651039123535156\n",
      "Step: 11, Loss: 2.9008166790008545, Accuracy: 0.15625, Computation time: 0.011220932006835938\n",
      "Step: 12, Loss: 2.099299907684326, Accuracy: 0.40625, Computation time: 0.011453866958618164\n",
      "Step: 13, Loss: 2.9311647415161133, Accuracy: 0.25, Computation time: 0.01322484016418457\n",
      "Step: 14, Loss: 2.986142158508301, Accuracy: 0.1875, Computation time: 0.6183681488037109\n",
      "Step: 15, Loss: 3.3710200786590576, Accuracy: 0.15625, Computation time: 0.011813879013061523\n",
      "Step: 16, Loss: 2.3758511543273926, Accuracy: 0.21875, Computation time: 0.011424064636230469\n",
      "Step: 17, Loss: 1.9757280349731445, Accuracy: 0.34375, Computation time: 0.012996196746826172\n",
      "Step: 18, Loss: 2.2317428588867188, Accuracy: 0.28125, Computation time: 0.01304316520690918\n",
      "Step: 19, Loss: 1.9508367776870728, Accuracy: 0.40625, Computation time: 0.011420965194702148\n",
      "Step: 20, Loss: 2.530996084213257, Accuracy: 0.3125, Computation time: 0.010982990264892578\n",
      "Step: 21, Loss: 1.5730146169662476, Accuracy: 0.5, Computation time: 0.011657953262329102\n",
      "Step: 22, Loss: 2.5358564853668213, Accuracy: 0.375, Computation time: 0.013261079788208008\n",
      "Step: 23, Loss: 2.1413612365722656, Accuracy: 0.3125, Computation time: 0.011379003524780273\n",
      "Step: 24, Loss: 1.3404314517974854, Accuracy: 0.5625, Computation time: 0.012735843658447266\n",
      "Step: 25, Loss: 1.6439722776412964, Accuracy: 0.34375, Computation time: 0.011379003524780273\n",
      "Step: 26, Loss: 2.492421865463257, Accuracy: 0.375, Computation time: 0.012196063995361328\n",
      "Step: 27, Loss: 2.1555888652801514, Accuracy: 0.4375, Computation time: 0.012993097305297852\n",
      "Step: 28, Loss: 1.5724995136260986, Accuracy: 0.46875, Computation time: 0.011373281478881836\n",
      "Step: 29, Loss: 1.5880123376846313, Accuracy: 0.53125, Computation time: 0.011488199234008789\n",
      "Step: 30, Loss: 1.8671646118164062, Accuracy: 0.53125, Computation time: 0.013269186019897461\n",
      "Step: 31, Loss: 1.6430108547210693, Accuracy: 0.4375, Computation time: 0.6334860324859619\n",
      "Step: 32, Loss: 1.365834355354309, Accuracy: 0.4375, Computation time: 0.011656999588012695\n",
      "Step: 33, Loss: 1.7181733846664429, Accuracy: 0.46875, Computation time: 0.013096809387207031\n",
      "Step: 34, Loss: 1.7498455047607422, Accuracy: 0.40625, Computation time: 0.011897802352905273\n",
      "Step: 35, Loss: 1.1364935636520386, Accuracy: 0.59375, Computation time: 0.011544942855834961\n",
      "Step: 36, Loss: 1.3664138317108154, Accuracy: 0.5625, Computation time: 0.01140284538269043\n",
      "Step: 37, Loss: 1.2726110219955444, Accuracy: 0.53125, Computation time: 0.01287698745727539\n",
      "Step: 38, Loss: 1.5625944137573242, Accuracy: 0.5, Computation time: 0.01110696792602539\n",
      "Step: 39, Loss: 0.8476220965385437, Accuracy: 0.75, Computation time: 0.013053178787231445\n",
      "Step: 40, Loss: 1.9646766185760498, Accuracy: 0.53125, Computation time: 0.011443138122558594\n",
      "Step: 41, Loss: 1.2348122596740723, Accuracy: 0.59375, Computation time: 0.011569023132324219\n",
      "Step: 42, Loss: 1.0851225852966309, Accuracy: 0.71875, Computation time: 0.011320114135742188\n",
      "Step: 43, Loss: 0.8216025829315186, Accuracy: 0.75, Computation time: 0.6169281005859375\n",
      "Step: 44, Loss: 1.4524753093719482, Accuracy: 0.59375, Computation time: 0.011818170547485352\n",
      "Step: 45, Loss: 1.2882490158081055, Accuracy: 0.65625, Computation time: 0.011432886123657227\n",
      "Step: 46, Loss: 1.0727267265319824, Accuracy: 0.71875, Computation time: 0.012801170349121094\n",
      "Step: 47, Loss: 1.0999048948287964, Accuracy: 0.625, Computation time: 0.011276006698608398\n",
      "Step: 48, Loss: 1.04422926902771, Accuracy: 0.5625, Computation time: 0.011444091796875\n",
      "Step: 49, Loss: 0.8854326009750366, Accuracy: 0.65625, Computation time: 0.012935876846313477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 256/256 [00:05<00:00, 47.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.425657272338867, Test Accuracy: 0.4921875\n"
     ]
    }
   ],
   "source": [
    "#%%memray_flamegraph\n",
    "eqx.clear_caches()\n",
    "jax.clear_caches()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd836e-74cb-47c0-9a00-dd6456e11601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
